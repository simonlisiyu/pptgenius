# LLM API配置
LLM_API_URL = "http://x.x.x.x:8000/v1/chat/completions"
LLM_API_KEY = "your-api-key-here"  # 请替换为实际的API密钥

# LLM模型配置
LLM_MODEL = "Qwen2.5-32B-Instruct-AWQ"

# LLM请求配置
LLM_REQUEST_CONFIG = {
    "temperature": 0.7,
    "max_tokens": 2000,
}

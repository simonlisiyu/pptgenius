# LLM API配置
LLM_API_URL = "http://x.x.x.x/v1/chat/completions"

# LLM模型配置
LLM_MODEL = "Qwen2.5-32B-Instruct-AWQ"

# LLM请求配置
LLM_REQUEST_CONFIG = {
    "temperature": 0.7,
    "max_tokens": 2000,
    "stream": True
}
